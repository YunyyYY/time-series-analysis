---
title: "Homework 1"
author: "Lingyun Guo"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
---


#### Question 1.1

\[
\begin{aligned}
\operatorname{Var}\left(\hat{\mu}\left(Y_{1: N}\right)\right) = \operatorname{Var}(\frac{1}{N} \sum_{n=1}^{N} y_{n}) &= \operatorname{Cov}(\frac{1}{N} \sum_{n=1}^{N} y_{n}, \frac{1}{N} \sum_{n=1}^{N} y_{n}) \\
&= \frac{1}{N^2}\operatorname{Cov}(\sum_{n=1}^{N} y_{n}, \sum_{n=1}^{N} y_{n})\\
&= \frac{1}{N^2}\operatorname{Cov}(\sum_{m=1}^{N} y_{m}, \sum_{n=1}^{N} y_{n})\\
&= \frac{1}{N^2}\sum_{m=1}^{N}\sum_{n=1}^{N}\operatorname{Cov}( y_{m},  y_{n})\\
&= \frac{1}{N^2}\left(\sum_{n=1}^N\operatorname{Cov}(y_n, y_n) + 2\sum_{h=1}^{N-1}\sum_{k=h+1}^{N}\operatorname{Cov}(y_h, y_k)\right)\\
&= \frac{1}{N^2}\left(N\gamma_0 + 2\sum_{h=1}^{N-1}\sum_{n=1}^{N-h}\operatorname{Cov}(y_h, y_{n+h}) \right) \\ 
&= \frac{1}{N^2}\left(N\gamma_0 + 2\sum_{h=1}^{N-1}(N-h)\gamma_h \right) \\
&= \frac{1}{N}\gamma_0 + \frac{2}{N^2}\sum_{h=1}^{N-1}(N-h)\gamma_h
\end{aligned}
\]



#### Question 1.2

We need to find the mean and standard deviation for 
\[
\hat{\rho}_{h}\left(Y_{1: N}\right)=\frac{\frac{1}{N} \sum_{n=1}^{N-h} Y_{n} Y_{n+h}}{\frac{1}{N} \sum_{n=1}^{N} Y_{n}^{2}},
\]
where $Y_1, ..., Y_N$ are IID random variables with zero mean and finite variance. As $N$ becomes large, $h \ll N$ and $\frac{1}{N} \sum_{n=1}^{N-h} Y_{n} Y_{n+h} \rightarrow \frac{1}{N} \sum_{n=1}^{N} Y_{n} Y_{n+h}$. 

Let $\hat{\gamma_h} = \frac{1}{N} \sum_{n=1}^{N} Y_{n} Y_{n+h}$ and $\hat{\gamma_0} = \frac{1}{N} \sum_{n=1}^{N} Y_{n}^{2}$.

Denote $\operatorname{Var}Y = \sigma^2$, since $\operatorname{Var}Y_n = \operatorname{Cov}(Y_n, Y_n) = \gamma_0$ and $\mu = 0$, 
\[
\mathbb{E}[\hat\gamma_0] = \mathbb{E}[\frac{1}{N} \sum_{n=1}^{N}\operatorname{Var}Y_n] = \frac{1}{N}\cdot N\cdot\mathbb{E}[\operatorname{Var}Y] = \sigma^2,
\]
and
\[
\begin{aligned}
\operatorname{Var}\hat\gamma_0 = \mathbb{E}[\hat\gamma_0^2] -  \mathbb{E}[\hat\gamma_0]^2 
&= \frac{1}{N^2} \mathbb{E}[(Y_1^2+...+Y_n^2)(Y_1^2+...+Y_n^2)] - \frac{1}{N^2}\mathbb{E}[\sum_{n=1}^NY_n^2]^2 \\
&= \frac{1}{N^2} \mathbb{E}[\sum_{n=1}^NY_n^4] + \frac{1}{N^2}\sum_{n=1}^N\sum_{m\neq n}\mathbb{E}[Y_n^2]\mathbb{E}[Y_m^2] - \frac{1}{N^2}\mathbb{E}[\sum_{n=1}^NY_n^2]^2 \\
&= \frac{1}{N}\mathbb{E}[Y^4] + \frac{N(N-1)}{N^2}\sigma^4 - \sigma^4 \\
&= \frac{3\sigma^4}{N}-\frac{1}{N}\sigma^4 \\
&= \frac{2\sigma^4}{N}
\end{aligned}
\]

Since $Y_1, ..., Y_N$ are independent, $\operatorname{Cov}(Y_m, Y_n) = 0$ for any $m, n\in\{1, ...N\}$. Therefore, 
\[\mathbb{E}(\hat\gamma_h) = 0.\]
and
\[
\begin{aligned}
\operatorname{Var}\hat\gamma_h &= \frac{1}{N^2}\mathbb{E}[(\sum_{n=1}^{N} Y_{n} Y_{n+h})^2] \\
&= \frac{1}{N^2}\sum_{n=1}^{N}\mathbb{E}[Y_n^2]\mathbb{E}[Y_{n+h}^2]\\
&= \frac{\sigma^4}{N}
\end{aligned}
\]

Define $\binom{\hat\gamma_0}{\hat\gamma_h}$, using central limit theorem for multidimensional variables, we have
\[
\sqrt{N} \left[\binom{\hat\gamma_0}{\hat\gamma_h} - \binom{\sigma^2}{0}\right] \sim \mathcal{N}(0, \Sigma),
\]
where $\Sigma = \operatorname{diag}(2\sigma^4/N, \sigma^4/N)$.

Define $g\binom{\hat\gamma_0}{\hat\gamma_h} = \hat\gamma_h/\hat\gamma_0$, by differentiation, 
\[
\nabla g\binom{\hat\gamma_0}{\hat\gamma_h}= \binom{-\frac{\hat\gamma_h}{\hat\gamma_0^{2}}}{\frac{1}{\hat\gamma_0}}.
\]
According to the multivariate delta method, for a mutidimensional estimator $B$ with mean $\beta$ and covariance matrix $\Sigma$, and if under central limit theorem satisfies $\sqrt{n}(B-\beta) \stackrel{D}{\longrightarrow} N(0, \Sigma)$, given a function $g(B)$, we can estimate $g(B) \approx g(\beta)+\nabla g(\beta)^\top \cdot(B-\beta)$ and
\[
\begin{aligned}
\mu_B &\approx g(\beta), \\ 
\sigma_B^2 &\approx \nabla g(\beta)^{\top} \cdot(\Sigma) \cdot \nabla g(\beta)
\end{aligned}
\]
In our case, 
\[
\begin{aligned}
\mu_\rho &= g\binom{\sigma^2}{0} = 0,\\
\sigma_\rho^2 &= [0, 1/\sigma^2]\left[\begin{array}{cc}{2\sigma^{4}/N} & {0} \\ {0} & {\sigma^{4}/N}\end{array}\right]\left[\begin{array}{l}{0} \\ {1/\sigma^2}\end{array}\right] = \frac{1}{N}
\end{aligned}
\]
Therefore, $1/\sqrt{N}$ is a reasonable approximation for the standard deviation of the null hypothesis distribution.

In conclusion, the dashed lines defines a $95\%$ confidence interval for the sampled $Y_i$ values to fall within if $Y_i$s follow a IID distribution. If $Y_i$s is indeed a sequence of IID, zero-mean random variables, then there should be about $95\%$ points fall between the dashed lines.


#### Question 1.3

References for question 1.2:

1. [Multivariate delta method](https://en.wikipedia.org/wiki/Delta_method#Multivariate_delta_method), Wikipedia. 
2. [Multidimensional CLT](https://en.wikipedia.org/wiki/Central_limit_theorem#Multidimensional_CLT), Wikipedia. 
3. St√©phane Guerrier, Roberto Molinari, Haotian Xu and Yuming Zhang, *Applied Time Series Analysis with R*. Chapter 3, [Fundamental Properties of Time Series](https://smac-group.github.io/ts/fundtimeseries.html), [Theorem 3.1](https://smac-group.github.io/ts/proofs.html#appendixa).

The method for proving $1/\sqrt{N}$ as a reasonable representation of the standard deviation is based on the proof of [*Reference 3*](https://smac-group.github.io/ts/proofs.html#appendixa), and the properties of central limit theorem and delta method for multidimensional random variables are refered from Wikipedia.