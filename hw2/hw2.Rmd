---
title: "Homework 2"
author: "Lingyun Guo"
output:
  pdf_document: default
  html_notebook: default
---

### Question 2.1

#### A
For $h>0$, 
\[
\begin{aligned}
\gamma_h &= \operatorname{Cov}(X_n, X_{n+h}) \\
&= \operatorname{Cov}(X_n, \phi X_{n+h-1}+\epsilon_{n+h}) \\
&= \operatorname{Cov}(X_n, \phi X_{n+h-1}) + \operatorname{Cov}(X_n, \epsilon_{n+h}) \\
&= \phi\gamma_{h-1} + \operatorname{Cov}(X_n, \epsilon_{n+h})
\end{aligned}
\]
Since $h>0$, $X_{n+h-1}$ is not dependent on $\epsilon_{n+h}$. Since $\epsilon$ is an IID distribution, $\epsilon_{n+h}$ is not dependent on $X_{n+h-1}$. Thus $\operatorname{Cov}(X_n, \epsilon_{n+h}) = 0$ and $\gamma_h = \phi\gamma_{h-1}$. Since \[
\begin{aligned}
\gamma_0 &= \operatorname{Cov}(X_n, X_{n}) \\
&= \operatorname{Cov}(\phi X_{n-1}+\epsilon_{n}, \phi X_{n-1}+\epsilon_{n}) \\
&= \phi^2\operatorname(Cov)(X_{n-1}, X_{n-1}) + 2\operatorname{Cov}(\phi X_{n-1}, \epsilon_n) + \operatorname{Cov}(\epsilon_n, \epsilon_n) \\
&= \phi^2\operatorname{Cov}(X_n, X_n) + \sigma^2 \\ 
&= \phi^2\gamma_0 + \sigma^2
\end{aligned}
\]

Thus, $\gamma_0 = \frac{\sigma^2}{1-\phi^2}$, and $\gamma_h = \frac{\sigma^2}{1-\phi^2}\phi^h$.

#### B
We can recursively write $X_n$ as a summation of $\epsilon$:
\[
\begin{aligned}
X_n &= \phi X_{n-1} + \epsilon_n \\
&= \phi(\phi X_{n-2} + \epsilon_{n-1}) + \epsilon_n \\
&= \dots \\
&= \epsilon_n + \phi\epsilon_{n-1} + \phi^2\epsilon_{n-2} + \dots \\
&= \sum_{k=0}^\infty\phi^k\epsilon_{n-k}
\end{aligned}
\]
Since $M_n = \mathrm{MA}(\infty) = \sum_{k=0}^\infty\theta_k\epsilon_{n-k}$, where $\theta_0 = 1$, the autocovariance function
\[
\begin{aligned}
\gamma_h &= \operatorname{Cov}(M_n, M_{n+h}) = \operatorname{Cov}(\sum_{k=0}^\infty\theta_k\epsilon_{n-k}, \sum_{l=0}^\infty\theta_l\epsilon_{n+h-l}) \\
&= \sum_{k=0}^\infty\theta_k\operatorname{Cov}(\epsilon_{n-k}, \sum_{l=0}^\infty\theta_l\epsilon_{n+h-l}) \\
&= \sum_{k=0}^\infty\theta_k\sum_{l=0}^\infty\theta_l\operatorname{Cov}(\epsilon_{n-k}, \epsilon_{n+h-l}) \\
&= \sum_{k=0}^\infty\sum_{l=0}^\infty\theta_k\theta_l\operatorname{Cov}(\epsilon_{n-k}, \epsilon_{n+h-l})
\end{aligned}
\]
Since \[
\operatorname{Cov}(\epsilon_{n-k}, \epsilon_{n+h-l}) = \left\{
\begin{aligned}
\sigma^2, \quad k = l-h \\
0, \quad k \neq l-h
\end{aligned}
\right.
\]
The previous formula can be simplified as $\gamma_h = \sum_{k=0}^\infty\theta_k\theta_{k+h}\sigma^2$. Since we have previously written $X_n$ in a $\mathrm{MA}(\infty)$ representation, where $\theta_k = \phi^k$ for $X_n$, taken into this formula we have
\[
\gamma_h = \sum_{k=0}^\infty\theta_k\theta_{k+h}\sigma^2 = \sum_{k=0}^\infty\phi^{2k+h}\sigma^2 
= \sigma^2\phi^h\sum_{k=0}^\infty\phi^{2k} = \sigma^2\phi^h\frac{1}{1-\phi^2}
\]

#### C

Using the derivation from A and B, the ACF of AR$(1)$ is $\rho(h) = \gamma_h / \gamma_0 = \phi^h$. Taken into $\phi=0.6$ and define the result of the first 100 terms in the series as `acf1`, 
```{r}
N = 100
n <- 0:N
phi <- 0.6
acf1 <- phi**n
```
Using the `ARMAacf` function in R and defie the first 100 terms as `acf2`,
```{r}
acf2 <- ARMAacf(ar=0.6, lag.max=N)
```
We can check that all terms in the two series are equal:
```{r}
acf2 <- as.numeric(acf2) # acf2 is named numeric, need to convert to numeric first
all.equal(acf1, acf2)
```

### Question 2.2

Recursively,
\[
\begin{aligned}
X_n &= X_{n-1}+\epsilon_n \\
&= X_{n-2} + \epsilon_{n-1} + \epsilon_n \\
&= \dots \\
&= X_0 + \epsilon_1 + \epsilon_2 + ... + \epsilon_n \\
&= \sum_{k=1}^n\epsilon_k
\end{aligned}
\]
Therefore,
\[
\begin{aligned}
\gamma_{mn} =\operatorname{Cov}(X_m, X_n) &= \operatorname{Cov}( \sum_{k=1}^m\epsilon_k,  \sum_{l=1}^n\epsilon_l) \\
&=\sum_{k=1}^m\sum_{l=1}^n \operatorname{Cov}(\epsilon_k, \epsilon_l)
\end{aligned}
\]
Since $\{\epsilon_n\}$ is white noise with variance $\sigma^2$, $\operatorname{Cov}(\epsilon_k, \epsilon_l)$ is nonzeron only when $k=l$, and is $\sigma^2$ instead. The number of equal terms in the equation is determined by the smaller one of $m, n$. Thus, the equation can be simplified as
\[
\gamma_{mn} = \sum_{k=1}^{\min(m, n)}\sigma^2 = \min(m, n)\cdot\sigma^2
\]

### Question 2.3

Reference:

1. Shumway, Robert H. and Stoffer, David S., *Time Series Analysis and Its Applications*, Fourth Edition.

For question 2.1 and 2.2, I checked the result with textbook after I finished them. But for both questions I did them individually.


